import os
import numpy as np
import pandas as pd
import joblib 
import yfinance as yf 
from sklearn.preprocessing import MinMaxScaler 


SCALER_DIR = "scalers" 
os.makedirs(SCALER_DIR, exist_ok=True) 
CSV_DATA_DIR = "stock_data_csv" 

# List of features the model will use for training and prediction
FEATURES = [
    'Open', 'High', 'Low', 'Close', 'Volume', 
    'MACD', 'Signal', 
    'RSI', 
    'MA10', 'MA50', 
    'ATR', 
    '%K', '%D',
    'OBV', 
    'Volatility', 
    'Close_Diff_1', 
    'ROC_5' 
]

# Data Fetching 
def fetch_stock_data(ticker: str, period: str = None, start_date=None, end_date=None) -> pd.DataFrame:
    """
    Reads historical stock data for a given ticker from its local CSV file.
    Handles multi-level/unconventional headers potentially generated by yfinance download.
    Optionally filters by date range if start_date and/or end_date are provided.
    """
    print(f"Fetching data for {ticker} from local CSV...")
    filepath = os.path.join(CSV_DATA_DIR, f"{ticker.upper()}.csv")

    if not os.path.exists(filepath):
        print(f"Error: CSV file not found for {ticker} at {filepath}. Run the update script.")
        raise FileNotFoundError(f"Data file for ticker '{ticker}' not found. Please ensure data is downloaded.")

    try:
        data = pd.read_csv(
            filepath,
            skiprows=3, 
            header=None 
        )
       
        data.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
        data['Date'] = pd.to_datetime(data['Date'])
        data.set_index('Date', inplace=True)
      

        if data.empty:
            print(f"Warning: CSV file for {ticker} is empty after reading/header assignment.")
            return pd.DataFrame()

        if not isinstance(data.index, pd.DatetimeIndex):
            data.index = pd.to_datetime(data.index)
            print(f"Re-Converted index to DatetimeIndex for {ticker}.")
        data.sort_index(inplace=True)

   
        if start_date:
            start_date = pd.to_datetime(start_date).tz_localize(None)
            data = data[data.index >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date).tz_localize(None)
            data = data[data.index <= end_date]
      
        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_req = [col for col in required_cols if col not in data.columns]
        if missing_req:
            print(f"Error: CSV for {ticker} missing critical columns after manual assignment: {missing_req}. Available: {data.columns.tolist()}")
            raise ValueError(f"Data for {ticker} missing columns: {missing_req}")

        initial_nans = data.isnull().sum().sum()
        if initial_nans > 0:
            print(f"Info: Found {initial_nans} NaN(s) in {ticker}.csv. Applying ffill then bfill.")
            data.ffill(inplace=True)
            data.bfill(inplace=True)
            final_nans = data.isnull().sum().sum()
            if final_nans > 0:
                 print(f"Warning: {final_nans} NaN(s) remain in {ticker}.csv after fill. Replacing with 0.")
                 data.fillna(0, inplace=True)

        print(f"Data loaded successfully for {ticker} from CSV. Shape: {data.shape}")
        return data

    except FileNotFoundError as e:
        print(f"Error reading CSV for {ticker}: {e}")
        raise e
    except Exception as e:
        print(f"Error reading or processing CSV for {ticker}: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"Failed to process data for {ticker} from CSV.") from e


# --- Indicator Calculation ---
def calculate_technical_indicators(df: pd.DataFrame, atr_period=14, stoch_period=14, stoch_sma=3, vol_window=21, roc_period=5) -> pd.DataFrame:
    required_base_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
    if df.empty: raise ValueError("Input DataFrame is empty.")
    missing = [c for c in required_base_cols if c not in df.columns];
    if missing: raise ValueError(f"Missing required columns for indicators: {missing}")

    df_out = df.copy()
    print(f"Calculating technical indicators for df shape: {df_out.shape}")

    try:
        # MACD calculation
        ema12 = df_out['Close'].ewm(span=12, adjust=False).mean()
        ema26 = df_out['Close'].ewm(span=26, adjust=False).mean()
        df_out['MACD'] = ema12 - ema26
        df_out['Signal'] = df_out['MACD'].ewm(span=9, adjust=False).mean()

        # RSI calculation
        delta = df_out['Close'].diff()
        gain = delta.clip(lower=0).fillna(0)
        loss = -delta.clip(upper=0).fillna(0)
        avg_gain = gain.ewm(com=atr_period - 1, adjust=False).mean()
        avg_loss = loss.ewm(com=atr_period - 1, adjust=False).mean()
        rs = np.where(avg_loss == 0, np.inf, avg_gain / avg_loss) # Handle division by zero
        df_out['RSI'] = 100.0 - (100.0 / (1.0 + rs))
        # Update to assign back instead of using inplace=True
        df_out['RSI'] = df_out['RSI'].replace(np.inf, 100.0) # Set RSI to 100 if avg_loss was 0
        df_out['RSI'] = df_out['RSI'].bfill() # Use bfill() method directly

        # Moving Averages
        df_out['MA10'] = df_out['Close'].rolling(window=10).mean()
        df_out['MA50'] = df_out['Close'].rolling(window=50).mean()

        # ATR calculation
        high_low = df_out['High'] - df_out['Low']
        high_prev_close = abs(df_out['High'] - df_out['Close'].shift(1))
        low_prev_close = abs(df_out['Low'] - df_out['Close'].shift(1))
        tr = pd.concat([high_low, high_prev_close, low_prev_close], axis=1).max(axis=1, skipna=False)
        df_out['ATR'] = tr.ewm(span=atr_period, adjust=False).mean()

        # Stochastic Oscillator (%K and %D)
        low_min = df_out['Low'].rolling(window=stoch_period).min()
        high_max = df_out['High'].rolling(window=stoch_period).max()
        denominator = (high_max - low_min)
        df_out['%K'] = 100.0 * (df_out['Close'] - low_min) / denominator.replace(0, np.nan)
        df_out['%D'] = df_out['%K'].rolling(window=stoch_sma).mean()
        # Fill NaNs and clip results
        df_out['%K'] = df_out['%K'].bfill() 
        df_out['%D'] = df_out['%D'].bfill() 
        df_out['%K'] = df_out['%K'].clip(0, 100) 
        df_out['%D'] = df_out['%D'].clip(0, 100)

        # OBV calculation
        close_diff_sign = np.sign(df_out['Close'].diff()).fillna(0)
        volume_filled = df_out['Volume'].fillna(0)
        df_out['OBV'] = (close_diff_sign * volume_filled).cumsum()

        # Volatility calculation (rolling std dev of returns)
        daily_ret = df_out['Close'].pct_change().replace([np.inf, -np.inf], 0).fillna(0)
        df_out['Volatility'] = daily_ret.rolling(window=vol_window).std()

        # Momentum features
        df_out['Close_Diff_1'] = df_out['Close'].diff(1) # Simple difference
        df_out['ROC_5'] = df_out['Close'].pct_change(periods=roc_period).replace([np.inf, -np.inf], 0).fillna(0) * 100 # Rate of change %

        # Drop rows with NaNs created by rolling windows at the start
        initial_rows = len(df_out);
        df_out.dropna(inplace=True); 
        final_rows = len(df_out)
        if initial_rows > final_rows:
            print(f"Dropped {initial_rows - final_rows} rows with NaNs after indicator calculations.")

    except Exception as e:
        print(f"Error during indicator calculation: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"Failed to calculate indicators: {e}") from e

    if df_out.empty:
        print("Warning: DataFrame is empty after indicator calculation.")

    print(f"Indicator calculation complete. Final shape: {df_out.shape}")
    return df_out

# data preprocessing
def preprocess_data(df: pd.DataFrame, sequence_length: int = 60, validation_split: float = 0.2):

    print(f"Starting preprocessing for LSTM...")
    target_col = 'Close' 
    min_rows_needed = sequence_length + 10 

    if df.empty or df.shape[0] < min_rows_needed:
        raise ValueError(f"Not enough rows ({df.shape[0]}) for sequence length {sequence_length} + 10 target steps.")

    
    missing_features = [f for f in FEATURES if f not in df.columns];
    if missing_features:
        raise ValueError(f"DataFrame missing required FEATURES: {missing_features}.")
    df_features = df[FEATURES].copy()

    if df_features.isnull().values.any() or np.isinf(df_features.values).any():
        print("Warning: Found NaNs/Infs before sequence creation. Replacing with 0.")
        df_features = df_features.apply(pd.to_numeric, errors='coerce') 
        df_features = df_features.fillna(0) 
        data_np = np.nan_to_num(df_features.values) 
    else:
        data_np = df_features.values 

    close_index = FEATURES.index(target_col);
    x_all_list, y_all_list = [], [] 

    
    for i in range(sequence_length, len(data_np) - 10 + 1):
        x_all_list.append(data_np[i-sequence_length : i, :]) 
        y_all_list.append(data_np[i : i+10, close_index])    

    if not x_all_list:
        raise ValueError("Could not create sequences. Check data length.")

    try:
        x_all = np.array(x_all_list, dtype=np.float64)
        y_all = np.array(y_all_list, dtype=np.float64) 
    except ValueError as e:
        print("Error creating NumPy arrays from sequence lists. Check shapes.")
        raise ValueError(f"Inconsistent sequence shapes/types: {e}") from e
    print(f"Created sequences: x={x_all.shape}, y={y_all.shape}")

    # Split sequences into training and validation sets
    split_index = int(len(x_all) * (1 - validation_split)); 
    x_train, x_val = x_all[:split_index], x_all[split_index:]
    y_train, y_val = y_all[:split_index], y_all[split_index:]

    if x_train.size == 0 or x_val.size == 0:
        raise ValueError(f"Train ({x_train.shape}) or validation ({x_val.shape}) set empty after split.")
    print(f"Split data: Train x={x_train.shape}, Val x={x_val.shape}")

   
    # Feature Scaling (MinMaxScaler: scales data to [0, 1] range)
    feature_scaler = MinMaxScaler();
    n_samples_train, n_timesteps, n_features = x_train.shape;
    x_train_reshaped = x_train.reshape(-1, n_features)
    feature_scaler.fit(x_train_reshaped)
    x_train_scaled = feature_scaler.transform(x_train_reshaped).reshape(x_train.shape)
    x_val_reshaped = x_val.reshape(-1, n_features)
    x_val_scaled = feature_scaler.transform(x_val_reshaped).reshape(x_val.shape)
    
    close_scaler = MinMaxScaler();
    y_train_reshaped = y_train.reshape(-1, 1)
    close_scaler.fit(y_train_reshaped)
    y_train_scaled = close_scaler.transform(y_train_reshaped).reshape(y_train.shape)
    y_val_reshaped = y_val.reshape(-1, 1)
    y_val_scaled = close_scaler.transform(y_val_reshaped).reshape(y_val.shape)
    
    scalers = {'feature_scaler': feature_scaler, 'close_scaler': close_scaler}
   
    return x_train_scaled, y_train_scaled, x_val_scaled, y_val_scaled, scalers


# --- Scaler Saving and Loading ---
def save_scaler(ticker: str, scalers: dict):
    """Saves the feature and target scalers."""
    os.makedirs(SCALER_DIR, exist_ok=True)
    filepath = os.path.join(SCALER_DIR, f"{ticker}_scalers.save")
    try:
        joblib.dump(scalers, filepath) 
        print(f"Scalers dictionary saved successfully to {filepath}")
    except Exception as e:
        print(f"Error saving scalers to {filepath}: {e}")
        raise e

def load_scaler(ticker: str) -> dict:
    """Loads the saved scalers."""
    path = os.path.join(SCALER_DIR, f"{ticker}_scalers.save")
    if not os.path.exists(path): 
        raise FileNotFoundError(f"Scalers file not found for '{ticker}' at '{path}'. Train first.")
    print(f"Loading scalers dictionary from {path}")
    try:
        scalers = joblib.load(path) 
        if not isinstance(scalers, dict):
             raise TypeError("Loaded object is not a dictionary.")
        if 'feature_scaler' not in scalers or 'close_scaler' not in scalers:
             raise KeyError("Loaded dictionary missing 'feature_scaler' or 'close_scaler'.")
        if not hasattr(scalers['feature_scaler'], 'transform') or \
           not hasattr(scalers['close_scaler'], 'inverse_transform'):
             raise TypeError("Loaded objects don't look like valid scalers.")
        print("Scaler dictionary loaded and validated.")
        return scalers
    except Exception as e:
        print(f"Error loading or validating scalers from {path}: {e}")
        raise e